{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated Transformer\n",
    "\n",
    "* http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(\"../python/\")\n",
    "from transformer import *\n",
    "\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 11\n",
    "batch = 30\n",
    "n_batch = 20\n",
    "in_seq_len = 10\n",
    "dat = data_gen(V, batch, n_batch, in_seq_len, ctx = ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: copy 10 input integers\n",
    "out_seq_len = 10\n",
    "dropout = .1\n",
    "data = data_gen(V, batch, n_batch, in_seq_len, ctx = ctx)\n",
    "model = make_model(V, V, in_seq_len, out_seq_len, N = 2, dropout = .1, d_model = 128, ctx = ctx)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx = ctx)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 1e-4, 'beta1': 0.9, 'beta2': 0.98 , 'epsilon': 1e-9})\n",
    "loss = gluon.loss.KLDivLoss(from_logits = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(dat):\n",
    "    dd =  d\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src :\n",
      "[[ 1.  9.  4.  1.  9.  3.  6.  4.  2. 10.]\n",
      " [ 1.  9.  9. 10.  3.  6.  3.  9.  9.  6.]]\n",
      "trg :\n",
      "[[ 1.  9.  4.  1.  9.  3.  6.  4.  2.]\n",
      " [ 1.  9.  9. 10.  3.  6.  3.  9.  9.]]\n",
      "trg_y :\n",
      "[[ 9.  4.  1.  9.  3.  6.  4.  2. 10.]\n",
      " [ 9.  9. 10.  3.  6.  3.  9.  9.  6.]]\n"
     ]
    }
   ],
   "source": [
    "print('src :')\n",
    "print('{}'.format(dd.src[:2].asnumpy()))\n",
    "print('trg :')\n",
    "print('{}'.format(dd.trg[:2].asnumpy()))\n",
    "print('trg_y :')\n",
    "print('{}'.format(dd.trg_y[:2].asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-23 00:12:48,051 - transformer - INFO - Epoch Step: 0 Loss: 0.162606 Tokens per Sec: 2094.781793\n",
      "2019-05-23 00:12:49,477 - transformer - INFO - Epoch Step: 1 Loss: 0.160456 Tokens per Sec: 3997.028441\n",
      "2019-05-23 00:12:50,905 - transformer - INFO - Epoch Step: 2 Loss: 0.161431 Tokens per Sec: 2328.966069\n",
      "2019-05-23 00:12:52,236 - transformer - INFO - Epoch Step: 3 Loss: 0.158512 Tokens per Sec: 4158.784896\n",
      "2019-05-23 00:12:53,450 - transformer - INFO - Epoch Step: 4 Loss: 0.154697 Tokens per Sec: 4445.752466\n",
      "2019-05-23 00:12:54,689 - transformer - INFO - Epoch Step: 5 Loss: 0.153123 Tokens per Sec: 4031.247504\n",
      "2019-05-23 00:12:56,059 - transformer - INFO - Epoch Step: 6 Loss: 0.153150 Tokens per Sec: 4701.958821\n",
      "2019-05-23 00:12:57,404 - transformer - INFO - Epoch Step: 7 Loss: 0.151267 Tokens per Sec: 3911.732066\n",
      "2019-05-23 00:12:58,774 - transformer - INFO - Epoch Step: 8 Loss: 0.151548 Tokens per Sec: 3980.590518\n",
      "2019-05-23 00:13:00,166 - transformer - INFO - Epoch Step: 9 Loss: 0.150994 Tokens per Sec: 4329.297923\n",
      "2019-05-23 00:13:01,593 - transformer - INFO - Epoch Step: 10 Loss: 0.150510 Tokens per Sec: 3619.535214\n",
      "2019-05-23 00:13:02,944 - transformer - INFO - Epoch Step: 11 Loss: 0.149241 Tokens per Sec: 3736.376468\n",
      "2019-05-23 00:13:04,385 - transformer - INFO - Epoch Step: 12 Loss: 0.149240 Tokens per Sec: 3367.346145\n",
      "2019-05-23 00:13:05,659 - transformer - INFO - Epoch Step: 13 Loss: 0.148810 Tokens per Sec: 4666.329114\n",
      "2019-05-23 00:13:07,071 - transformer - INFO - Epoch Step: 14 Loss: 0.148797 Tokens per Sec: 3534.426561\n",
      "2019-05-23 00:13:08,406 - transformer - INFO - Epoch Step: 15 Loss: 0.147362 Tokens per Sec: 4665.906143\n",
      "2019-05-23 00:13:09,697 - transformer - INFO - Epoch Step: 16 Loss: 0.148609 Tokens per Sec: 2447.915646\n",
      "2019-05-23 00:13:11,010 - transformer - INFO - Epoch Step: 17 Loss: 0.146271 Tokens per Sec: 3821.418477\n",
      "2019-05-23 00:13:12,363 - transformer - INFO - Epoch Step: 18 Loss: 0.148487 Tokens per Sec: 4620.332920\n",
      "2019-05-23 00:13:13,572 - transformer - INFO - Epoch Step: 19 Loss: 0.147321 Tokens per Sec: 4505.787415\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    run_epoch(epoch, data_gen(V, batch, n_batch, in_seq_len, ctx = ctx), model, trainer, loss, ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = \n",
      "[[ 1.  5.  2.  3.  2.  5.  7.  8.  9. 10.]]\n",
      "<NDArray 1x10 @gpu(0)>\n",
      "tgt = \n",
      "[[ 1.  1.  2.  3.  2.  5.  7.  8.  9. 10.]]\n",
      "<NDArray 1x10 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = nd.array([[start_symbol]], ctx = ctx)\n",
    "    for i in range(max_len):\n",
    "        tgt_mask = subsequent_mask(ys.shape[1])\n",
    "        out = model.decode(memory, src_mask, ys, tgt_mask.as_in_context(ctx))\n",
    "        next_word = nd.argmax(out, axis = 2)\n",
    "        ys = nd.concat(ys, next_word[:,-1].expand_dims(axis = 1), dim = 1)\n",
    "    return ys\n",
    "\n",
    "src = nd.array([[1,5,2,3,2,5,7,8,9,10]], ctx = ctx)\n",
    "print('src = {}'.format(src))\n",
    "src_mask = nd.ones_like(src, ctx = ctx)\n",
    "with autograd.predict_mode():\n",
    "    res = greedy_decode(model, src, src_mask, max_len=9, start_symbol=1)\n",
    "print('tgt = {}'.format(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
